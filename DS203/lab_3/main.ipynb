{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Exercise: Hands-on and Interview based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands-on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, desc, explode, lit\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/12/04 14:09:23 WARN Utils: Your hostname, DESKTOP-7M8TID9 resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "24/12/04 14:09:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/04 14:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Hands-on\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creating DataFrames\n",
    "Create a DataFrame from the following list of dictionaries:\n",
    "\n",
    "```\n",
    "data = [{\"id\": 1, \"name\": \"Alice\", \"age\": 23},\n",
    "{\"id\": 2, \"name\": \"Bob\", \"age\": 27},\n",
    "{\"id\": 3, \"name\": \"Cathy\", \"age\": 22}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Alice\",\n",
    "        \"age\": 23,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Bob\",\n",
    "        \"age\": 27,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"Cathy\",\n",
    "        \"age\": 22\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "|age| id| name|\n",
      "+---+---+-----+\n",
      "| 23|  1|Alice|\n",
      "| 27|  2|  Bob|\n",
      "| 22|  3|Cathy|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Display the schema of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('age', LongType(), True), StructField('id', LongType(), True), StructField('name', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(df.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Selecting Data\n",
    "    * Select the name and age columns from the DataFrame.\n",
    "    * Add 5 years to the age column and create a new column named age_plus_5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 23|\n",
      "|  Bob| 27|\n",
      "|Cathy| 22|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"age_plus_5\", col(\"age\") + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+\n",
      "|age| id| name|age_plus_5|\n",
      "+---+---+-----+----------+\n",
      "| 23|  1|Alice|        28|\n",
      "| 27|  2|  Bob|        32|\n",
      "| 22|  3|Cathy|        27|\n",
      "+---+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Renaming Columns\n",
    "    * Rename the id column to user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"id\", \"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+----------+\n",
      "|age|user_id| name|age_plus_5|\n",
      "+---+-------+-----+----------+\n",
      "| 23|      1|Alice|        28|\n",
      "| 27|      2|  Bob|        32|\n",
      "| 22|      3|Cathy|        27|\n",
      "+---+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Dropping Columns\n",
    "    * Drop the user_id column from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+\n",
      "|age| name|age_plus_5|\n",
      "+---+-----+----------+\n",
      "| 23|Alice|        28|\n",
      "| 27|  Bob|        32|\n",
      "| 22|Cathy|        27|\n",
      "+---+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Distinct Values\n",
    "    * Find all distinct values in a column of your choice from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 23|\n",
      "| 27|\n",
      "| 22|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"age\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Basic Column Operations\n",
    "    * Create a new column that concatenates the name column with the string \"_student\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"name_student\", concat(col(\"name\"), lit(\"_student\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "|age| name|age_plus_5| name_student|\n",
      "+---+-----+----------+-------------+\n",
      "| 23|Alice|        28|Alice_student|\n",
      "| 27|  Bob|        32|  Bob_student|\n",
      "| 22|Cathy|        27|Cathy_student|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Filtering Rows\n",
    "    * Filter rows where age is greater than 25.\n",
    "    * Filter rows where the name starts with the letter \"A\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+------------+\n",
      "|age|name|age_plus_5|name_student|\n",
      "+---+----+----------+------------+\n",
      "| 27| Bob|        32| Bob_student|\n",
      "+---+----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"age\") > 25).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "|age| name|age_plus_5| name_student|\n",
      "+---+-----+----------+-------------+\n",
      "| 23|Alice|        28|Alice_student|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"name\").startswith(\"A\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Aggregations\n",
    "    * Count the total number of rows.\n",
    "    * Group by age and count the number of records for each age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 23|    1|\n",
      "| 27|    1|\n",
      "| 22|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Sorting\n",
    "    * Sort the DataFrame by name in ascending order.\n",
    "    * Sort the DataFrame by age in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "|age| name|age_plus_5| name_student|\n",
      "+---+-----+----------+-------------+\n",
      "| 23|Alice|        28|Alice_student|\n",
      "| 27|  Bob|        32|  Bob_student|\n",
      "| 22|Cathy|        27|Cathy_student|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "|age| name|age_plus_5| name_student|\n",
      "+---+-----+----------+-------------+\n",
      "| 27|  Bob|        32|  Bob_student|\n",
      "| 23|Alice|        28|Alice_student|\n",
      "| 22|Cathy|        27|Cathy_student|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Working with Null Values\n",
    "    * Identify rows with null values in any column.\n",
    "    * Replace null values in the age column with the average age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+------------+\n",
      "|age|name|age_plus_5|name_student|\n",
      "+---+----+----------+------------+\n",
      "+---+----+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(reduce(lambda a, b: a | b, [col(c).isNull() for c in df.columns])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_age = df.select(\"age\").na.drop().agg({\"age\": \"avg\"}).first()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna({\"age\": average_age})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------------+\n",
      "|age| name|age_plus_5| name_student|\n",
      "+---+-----+----------+-------------+\n",
      "| 23|Alice|        28|Alice_student|\n",
      "| 27|  Bob|        32|  Bob_student|\n",
      "| 22|Cathy|        27|Cathy_student|\n",
      "+---+-----+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Joining DataFrames\n",
    "Given two DataFrames:\n",
    "\n",
    "`df1 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\")]`\n",
    "\n",
    "`df2 = [(1, \"A\"), (2, \"B\"), (4, \"D\")]`\n",
    "\n",
    "Perform:\n",
    "- Inner join\n",
    "- Left join\n",
    "- Full outer join\n",
    "- Left semi join\n",
    "- Left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\")], [\"id\", \"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.createDataFrame([(1, \"A\"), (2, \"B\"), (4, \"D\")], [\"id\", \"letter\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|letter|\n",
      "+---+-----+------+\n",
      "|  1|Alice|     A|\n",
      "|  2|  Bob|     B|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=\"id\", how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|letter|\n",
      "+---+-----+------+\n",
      "|  1|Alice|     A|\n",
      "|  2|  Bob|     B|\n",
      "|  3|Cathy|  NULL|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=\"id\", how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "| id| name|letter|\n",
      "+---+-----+------+\n",
      "|  1|Alice|     A|\n",
      "|  2|  Bob|     B|\n",
      "|  3|Cathy|  NULL|\n",
      "|  4| NULL|     D|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=\"id\", how=\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  2|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=\"id\", how=\"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  3|Cathy|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on=\"id\", how=\"left_anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Pivot Tables\n",
    "    * Use pivoting to calculate the average age grouped by gender and occupation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 23, \"gender\": \"F\", \"occupation\": \"Engineer\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 27, \"gender\": \"M\", \"occupation\": \"Doctor\"},\n",
    "    {\"id\": 3, \"name\": \"Cathy\", \"age\": 22, \"gender\": \"F\", \"occupation\": \"Artist\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+--------+\n",
      "|gender|Artist|Doctor|Engineer|\n",
      "+------+------+------+--------+\n",
      "|     F|  22.0|  NULL|    23.0|\n",
      "|     M|  NULL|  27.0|    NULL|\n",
      "+------+------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"gender\").pivot(\"occupation\").agg({\"age\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Exploding Columns\n",
    "    * Given a column with lists, explode the list into individual rows:\n",
    "\n",
    "```\n",
    "data = [{\"id\": 1, \"values\": [10, 20, 30]}, {\"id\": 2, \"values\": [40, 50]}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\"id\": 1, \"values\": [10, 20, 30]}, {\"id\": 2, \"values\": [40, 50]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  1|   10|\n",
      "|  1|   20|\n",
      "|  1|   30|\n",
      "|  2|   40|\n",
      "|  2|   50|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", explode(col(\"values\")).alias(\"value\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Union DataFrames\n",
    "    * Combine two DataFrames vertically using union. Ensure both DataFrames have the same schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([(1, \"Alice\", 23), (2, \"Bob\", 27)], [\"id\", \"name\", \"age\"])\n",
    "df2 = spark.createDataFrame([(3, \"Cathy\", 22), (4, \"David\", 30)], [\"id\", \"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+\n",
      "| id| name|age|\n",
      "+---+-----+---+\n",
      "|  1|Alice| 23|\n",
      "|  2|  Bob| 27|\n",
      "|  3|Cathy| 22|\n",
      "|  4|David| 30|\n",
      "+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview based:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is PySpark? How does it differ from Pandas?\n",
    "    * Discuss the differences in terms of scalability, distributed computing, and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark is the Python API for Apache Spark, designed for big data processing and distributed computing, allowing users to handle large datasets across a cluster of machines. In contrast, Pandas is an in-memory data manipulation library suitable for smaller datasets, operating on a single machine. While PySpark excels in scalability and parallel processing for big data applications, Pandas is ideal for data analysis and exploration tasks on manageable datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the advantages of using PySpark over traditional Python frameworks?\n",
    "    * Mention aspects like fault tolerance, in-memory computation, and support for big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark offers significant advantages over traditional Python frameworks, particularly for big data processing. It provides fault tolerance through Resilient Distributed Datasets (RDDs), enabling automatic recovery from failures. Its in-memory computation speeds up data processing, while its ability to handle large datasets across distributed systems enhances scalability. Additionally, PySpark integrates well with the Hadoop ecosystem and includes a user-friendly DataFrame API and MLlib for machine learning, making it a robust choice for data analysis and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain the concept of Resilient Distributed Dataset (RDD). How does it differ from DataFrames?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resilient Distributed Dataset (RDD) is a fundamental data structure in Apache Spark that represents an immutable, fault-tolerant collection of objects distributed across a cluster. RDDs allow for parallel processing and lazy evaluation but require more manual optimization and lack a schema. In contrast, DataFrames provide a higher-level abstraction with a defined schema, enabling more efficient execution through Spark's Catalyst optimizer and a more user-friendly API for structured data manipulation. Overall, DataFrames are generally preferred for most data processing tasks due to their performance and ease of use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is lazy evaluation in PySpark? Why is it useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy evaluation in PySpark is a strategy that delays the execution of data transformations until an action is called. This approach allows for optimization of the entire computation plan, efficient resource management, and improved fault tolerance. By building a logical plan of transformations, PySpark can reduce unnecessary computations and enhance the performance of data processing pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain the difference between transformations and actions in PySpark.\n",
    "    * Provide examples of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations are operations that create a new RDD (Resilient Distributed Dataset) from an existing one. They are lazy, meaning they do not compute their results immediately. Instead, they build up a logical plan of transformations to be executed when an action is called. Common transformations include map, filter, flatMap, groupByKey, and reduceByKey.\n",
    "\n",
    "Example of a Transformation:\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Transformation Example\")\n",
    "\n",
    "# Create an RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Transformation: Map to square each element\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "\n",
    "# Note: The computation is not executed yet.\n",
    "```\n",
    "\n",
    "Actions are operations that trigger the execution of the transformations and return a value to the driver program or write data to an external storage system. Actions include operations like collect, count, first, take, and saveAsTextFile.\n",
    "Example of an Action:\n",
    "```python\n",
    "# Action: Collect the results to the driver\n",
    "result = squared_rdd.collect()\n",
    "\n",
    "# This will execute the transformations and return the squared values\n",
    "print(result)  # Output: [1, 4, 9, 16, 25]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How is the join() operation optimized in PySpark? What are the different types of joins PySpark supports?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, the join() operation is optimized through techniques like broadcast joins, partitioning, sort-merge joins, skew handling, and the Catalyst optimizer. It supports several types of joins, including inner, outer, left, right, cross, semi, and anti joins, allowing for flexible data manipulation. These optimizations and join types enhance performance and enable efficient handling of large datasets in distributed environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What are the key differences between PySpark DataFrames and SQL tables? Can you use SQL queries on PySpark DataFrames?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark DataFrames are distributed collections of data organized into named columns, optimized for performance in a distributed computing environment, while SQL tables are structured data stored in relational databases. You can manipulate PySpark DataFrames using Python APIs, whereas SQL tables are accessed through SQL queries. Additionally, PySpark allows you to run SQL queries on DataFrames by registering them as temporary views, enabling a seamless integration of SQL with PySpark's capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Explain the role of SparkSession in PySpark. Why is it needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession in PySpark is the main entry point for working with Spark, providing a unified interface for accessing Spark's features, such as DataFrames and SQL queries. It simplifies configuration management and automatically creates a SparkContext, which is essential for connecting to a Spark cluster. Overall, SparkSession streamlines the development process by consolidating various functionalities into a single object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. What is the difference between SparkContext and SparkSession? When to use each of them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkContext is the entry point for low-level Spark operations, primarily used for working with RDDs. In contrast, SparkSession is a higher-level entry point that encapsulates SparkContext and is designed for working with structured data using DataFrames and Datasets, as well as for executing SQL queries. It is recommended to use SparkSession for most applications due to its enhanced functionality and ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
